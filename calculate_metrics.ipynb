{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import redis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to redis\n",
    "\n",
    "secret_file_path = \"\"\n",
    "backend = redis.Redis(\n",
    "    charset=\"utf-8\",\n",
    "    decode_responses=True,\n",
    "    password=open(secret_file_path).read().strip(),\n",
    "    port=6666,\n",
    ")\n",
    "key_values = {key: json.loads(backend.get(key)) for key in backend.keys()}\n",
    "\n",
    "OUTPUT_DIR = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve user data from database and save as jsonl\n",
    "\n",
    "# Filter user ids during debugging\n",
    "user_ids = [k for k in key_values if \":user:\" in k and \"debug\" not in k]\n",
    "user_data = []\n",
    "\n",
    "for user in user_ids:\n",
    "    tmp = json.loads(backend[user])\n",
    "\n",
    "    experiment_id = user.split(\":\")[0]\n",
    "    user_id = user.split(\":\")[2]\n",
    "    if len(tmp['split_id']) > 1: split_id = int(tmp['split_id'][0])\n",
    "    else: split_id = int(tmp['split_id'])\n",
    "    \n",
    "    if 'consent-check' in tmp: consent_check = tmp['consent-check']\n",
    "    else: consent_check = False\n",
    "\n",
    "    if 'attention-check' in tmp: attention_check = tmp['attention-check']\n",
    "    else: attention_check = False\n",
    "    if 'attention-state' in tmp: attention_state = tmp['attention-state']\n",
    "    else: attention_state = False\n",
    "\n",
    "    if 'user-task-instances' in tmp: user_task_instances = tmp['user-task-instances']\n",
    "    else: user_task_instances = False\n",
    "\n",
    "    if 'task-state' in tmp: task_state = tmp['task-state']\n",
    "    else: task_state = False\n",
    "\n",
    "    if 'survey-check' in tmp: survey_check = tmp['survey-check']\n",
    "    else: survey_check = False\n",
    "    if 'survey-state' in tmp: survey_state = tmp['survey-state']\n",
    "    else: survey_state = False\n",
    "\n",
    "    user_data.append(\n",
    "        {\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"user_id\": user_id,\n",
    "            \"split_id\": split_id,\n",
    "            \"consent_check\": consent_check,\n",
    "            \"attention_check\": attention_check,\n",
    "            \"attention_state\": attention_state,\n",
    "            \"user_task_instances\": user_task_instances,\n",
    "            \"task_state\": task_state,\n",
    "            \"survey_check\": survey_check,\n",
    "            \"survey_state\": survey_state,\n",
    "        }\n",
    "    )\n",
    "\n",
    "user_df = pd.DataFrame(user_data)\n",
    "user_data_path = \"\"\n",
    "user_df.to_json(os.path.join(OUTPUT_DIR, user_data_path), lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter old data and include only experiment data\n",
    "\n",
    "df = pd.read_json(user_data_path, lines=True)\n",
    "\n",
    "# This line excludes rows with the following criteria:\n",
    "# 1. attention_state, task state, survey state are false (we only want complete data)\n",
    "# 2. remove user ids\n",
    "filtered_df = df[(~df['user_id'].str.startswith('0')) & \\\n",
    "                 ~(df['attention_state'] == False) & \\\n",
    "                 ~(df['task_state'] == False) & \\\n",
    "                 ~(df['survey_state'] == False)]\n",
    "\n",
    "print(f\"len of filtered_df: {len(filtered_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_senstitive_attribute_dict(key):\n",
    "    tmp = json.loads(backend[key])\n",
    "    assert len(tmp) == 200\n",
    "    dataset = key.split(\":\")[0].split(\"-\")[0]\n",
    "    \n",
    "    d = {}\n",
    "    for i in tmp:\n",
    "        sensitive_attribute = ''\n",
    "        # gender for German Credit and race for rcdv\n",
    "        if dataset == 'rcdv':\n",
    "            sensitive_attribute = i['features']['white']\n",
    "        if dataset == 'german':\n",
    "            if i['features']['personal-status-sex_1'] == 1 or i['features']['personal-status-sex_3'] == 1:\n",
    "                sensitive_attribute = 1 # male\n",
    "            if i['features']['personal-status-sex_2'] == 1 or i['features']['personal-status-sex_5'] == 1:\n",
    "                sensitive_attribute = 0 # female\n",
    "        instance_id = i['instance-idx']\n",
    "        d[instance_id] = sensitive_attribute\n",
    "    return d\n",
    "\n",
    "\n",
    "# Create task data senstive attributes dict\n",
    "# to retrieve sensitive attributes more easily\n",
    "\n",
    "task_data = [k for k in key_values if \"task-data\" in k and \"debug\" not in k]\n",
    "task_data_dict = {}\n",
    "for key in task_data:\n",
    "    tmp_d = create_senstitive_attribute_dict(key)\n",
    "    experiment_id = key.split(\":\")[0]\n",
    "    task_data_dict[experiment_id] = tmp_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each user label as a row in a dataframe\n",
    "\n",
    "user_ids = set()\n",
    "\n",
    "task_data = []\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    task_labels = row['task_state']['labels']\n",
    "    try:\n",
    "        assert len(task_labels) == 20\n",
    "    except:\n",
    "        print(f\"user id: {row['user_id']}\")\n",
    "\n",
    "    # Get time label on last task label\n",
    "    task_keys = list(task_labels.keys())\n",
    "    last_task_key = task_keys[len(task_keys)-1]\n",
    "    last_task_end_time = task_labels[last_task_key]['label-time']\n",
    "\n",
    "    # Get time label on last attention label\n",
    "    attention_labels = row['attention_state']['labels']\n",
    "    attention_qns_keys = list(attention_labels.keys())\n",
    "    last_attention_qn_key = attention_qns_keys[len(attention_qns_keys)-1]\n",
    "    attention_end_time = attention_labels[last_attention_qn_key]['label-time']\n",
    "\n",
    "    # Find total time taken and average time taken in seconds\n",
    "    # Divide by 100 since original timestamps are in milliseconds\n",
    "    total_time_taken_for_task = (last_task_end_time - attention_end_time) / 100 / 60\n",
    "    average_time_per_prediction = total_time_taken_for_task / len(task_labels)\n",
    "\n",
    "    count = 0\n",
    "    for index, task_label in task_labels.items():\n",
    "        # Use only the first 20 labels\n",
    "        if count > 20:\n",
    "            break\n",
    "\n",
    "        if 'instance' not in task_label:\n",
    "            break\n",
    "\n",
    "        instance_id = int(task_label['instance'])\n",
    "        experiment_id = row['experiment_id']\n",
    "\n",
    "        user_label = 0\n",
    "        try:\n",
    "            if task_label['label'] == \"will_recidivate\" or task_label['label'] == \"good\":\n",
    "                user_label = 1\n",
    "        except:\n",
    "            user_ids.add(row['user_id'])\n",
    "            break\n",
    "        actual_label = int(task_label['actual_label'])\n",
    "        prediction = int(task_label['prediction'])\n",
    "\n",
    "        task_data.append(\n",
    "            {\n",
    "                \"experiment_id\": experiment_id,\n",
    "                \"user_id\": row['user_id'],\n",
    "                \"split_id\": row['split_id'],\n",
    "                \"instance_order\": index,\n",
    "                \"instance_id\": instance_id,\n",
    "                \"user_label\": user_label,\n",
    "                \"actual_label\": actual_label,\n",
    "                \"prediction\": prediction,\n",
    "                \"sensitive_attribute\": task_data_dict[experiment_id][instance_id],\n",
    "                \"average_time\": average_time_per_prediction\n",
    "            }\n",
    "        )\n",
    "        count += 1\n",
    "\n",
    "task_df = pd.DataFrame(task_data)\n",
    "task_data_path = \"\"\n",
    "task_df.to_json(os.path.join(OUTPUT_DIR, task_data_path), lines=True, orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze task data with evaluation metrics\n",
    "\n",
    "def calculate_tpr_fpr(y_true, y_pred):\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape == (1, 1):\n",
    "        # Only one class present in y_true\n",
    "        tn, fp, fn, tp = 0, 0, 0, cm[0, 0]\n",
    "    elif cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected confusion matrix shape: {}\".format(cm.shape))\n",
    "    \n",
    "    # Avoid division by zero when calculating TPR and FPR\n",
    "    tpr = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    fpr = fp / (fp + tn) if fp + tn > 0 else 0\n",
    "    return round(tpr, 3), round(fpr, 3)\n",
    "\n",
    "\n",
    "def absolute_equality_of_opportunity_difference(y_true, y_pred, sensitive_attr):\n",
    "    group_0_indices = (sensitive_attr == 0)\n",
    "    group_1_indices = (sensitive_attr == 1)\n",
    "    \n",
    "    tpr_0, _ = calculate_tpr_fpr(y_true[group_0_indices], y_pred[group_0_indices])\n",
    "    tpr_1, _ = calculate_tpr_fpr(y_true[group_1_indices], y_pred[group_1_indices])\n",
    "    \n",
    "    return np.abs(tpr_0 - tpr_1)\n",
    "\n",
    "\n",
    "def absolute_equalized_odds_difference(y_true, y_pred, sensitive_attr):\n",
    "    group_0_indices = (sensitive_attr == 0)\n",
    "    group_1_indices = (sensitive_attr == 1)\n",
    "    \n",
    "    tpr_0, fpr_0 = calculate_tpr_fpr(y_true[group_0_indices], y_pred[group_0_indices])\n",
    "    tpr_1, fpr_1 = calculate_tpr_fpr(y_true[group_1_indices], y_pred[group_1_indices])\n",
    "    \n",
    "    return (np.abs(tpr_0 - tpr_1) + np.abs(fpr_0 - fpr_1)) / 2\n",
    "\n",
    "\n",
    "conditions = task_df.apply(lambda row: f\"{row['experiment_id']}_{row['split_id']}\", axis=1).unique()\n",
    "objective_results = \"\"\n",
    "with open(objective_results, \"w\") as file:\n",
    "    file.write(f\"total conditions: {len(conditions)}\\n\\n\")\n",
    "\n",
    "    for condition in conditions:\n",
    "        experiment_id = condition.split('_')[0]\n",
    "        split_id = condition.split('_')[1]\n",
    "        tmp_df = task_df[(task_df['experiment_id'] == experiment_id) & (task_df['split_id'] == int(split_id))]\n",
    "\n",
    "        time_df = tmp_df.groupby(['experiment_id', 'split_id','user_id'])['average_time'].mean().reset_index(name='average_time_per_prediction')\n",
    "        time_taken = np.mean(time_df['average_time_per_prediction'])\n",
    "\n",
    "        over_reliance = round(len(tmp_df[(tmp_df['user_label'] == tmp_df['prediction']) & (tmp_df['actual_label'] != tmp_df['prediction'])]) / len(tmp_df), 3)\n",
    "        under_reliance = round(len(tmp_df[(tmp_df['user_label'] != tmp_df['prediction']) & (tmp_df['actual_label'] == tmp_df['prediction'])]) / len(tmp_df), 3)\n",
    "\n",
    "        preds = tmp_df['prediction']\n",
    "        labels = tmp_df['actual_label']\n",
    "        user_labels = tmp_df['user_label']\n",
    "        sensitive_attr = tmp_df['sensitive_attribute']\n",
    "        assert len(preds) == len(labels) == len(user_labels) == len(tmp_df)\n",
    "        \n",
    "        accuracy = round(metrics.accuracy_score(labels, preds), 3)\n",
    "        precision = round(metrics.precision_score(labels, preds), 3)\n",
    "        recall = round(metrics.recall_score(labels, preds), 3)\n",
    "        f1 = round(metrics.f1_score(labels, preds), 3)\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(labels, preds).ravel()\n",
    "\n",
    "        tpr, fpr = calculate_tpr_fpr(labels, preds)\n",
    "        aeood = round(absolute_equality_of_opportunity_difference(labels, preds, sensitive_attr), 3)\n",
    "        aeod = round(absolute_equalized_odds_difference(labels, preds, sensitive_attr), 3)\n",
    "\n",
    "        file.write(f\"===== {experiment_id}:{split_id} ======\\n\")\n",
    "        file.write(f\"# of participants: {len(preds) / 20} (we want it to be around 30)\\n\")\n",
    "        file.write(f\"# of preds/labels: {len(preds)} (we want it to be around 600)\\n\")\n",
    "        file.write(f\"accuracy: {accuracy}\\n\")\n",
    "        file.write(f\"precision: {precision}\\n\")\n",
    "        file.write(f\"recall: {recall}\\n\")\n",
    "        file.write(f\"f1: {f1}\\n\")\n",
    "        file.write(f\"over reliance: {over_reliance}\\n\")\n",
    "        file.write(f\"under reliance: {under_reliance}\\n\")\n",
    "        file.write(f\"tpr: {tpr}\\n\")\n",
    "        file.write(f\"fpr: {fpr}\\n\")\n",
    "        file.write(f\"absolute equality of opportunity difference: {aeood}\\n\")\n",
    "        file.write(f\"absolute equalized odds difference: {aeod}\\n\")\n",
    "        file.write(f\"true negatives: {tn}\\n\")\n",
    "        file.write(f\"false positives: {fp}\\n\")\n",
    "        file.write(f\"false negatives: {fn}\\n\")\n",
    "        file.write(f\"true positives: {tp}\\n\")\n",
    "        file.write(f\"time taken per prediction in minutes: {time_taken}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each survey label as a row in a dataframe\n",
    "\n",
    "likert_score_conversion = {\n",
    "    \"Strongly Disagree\": 1,\n",
    "    \"Disagree\": 2,\n",
    "    \"Neutral\": 3,\n",
    "    \"Agree\": 4,\n",
    "    \"Strongly Agree\" : 5,\n",
    "}\n",
    "\n",
    "survey_data = []\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    survey_labels = row['survey_state']['labels']\n",
    "\n",
    "    for index, survey_label in survey_labels.items():\n",
    "        instance_id = int(task_label['instance'])\n",
    "        experiment_id = row['experiment_id']\n",
    "        survey_label = survey_label['label']\n",
    "\n",
    "        survey_data.append(\n",
    "            {\n",
    "                \"experiment_id\": experiment_id,\n",
    "                \"user_id\": row['user_id'],\n",
    "                \"split_id\": row['split_id'],\n",
    "                \"question_number\": index,\n",
    "                \"survey_label\": survey_label,\n",
    "                \"survey_label_score\": likert_score_conversion.get(survey_label, False),\n",
    "            }\n",
    "        )\n",
    "\n",
    "survey_df = pd.DataFrame(survey_data)\n",
    "survey_data_path = \"\"\n",
    "survey_df.to_json(os.path.join(OUTPUT_DIR, survey_data_path), lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze survey data\n",
    "\n",
    "conditions = survey_df.apply(lambda row: f\"{row['experiment_id']}_{row['split_id']}\", axis=1).unique()\n",
    "subjective_results = \"\"\n",
    "with open(subjective_results, \"w\") as file:\n",
    "    file.write(f\"total conditions: {len(conditions)}\\n\\n\")\n",
    "\n",
    "    for condition in conditions:\n",
    "        experiment_id = condition.split('_')[0]\n",
    "        split_id = condition.split('_')[1]\n",
    "        condition_df = survey_df[(survey_df['experiment_id'] == experiment_id) & (survey_df['split_id'] == int(split_id))]\n",
    "        file.write(f\"===== {experiment_id}:{split_id} ======\\n\")\n",
    "\n",
    "        survey_qns = [i for i in range(1, 17)]\n",
    "        if split_id == '1': \n",
    "            survey_qns = [i for i in range(1, 17) if i not in range(2, 17)]\n",
    "        if split_id == '2': \n",
    "            survey_qns = [i for i in range(1, 17) if i not in [4, 10, 11, 12, 13, 14]]\n",
    "\n",
    "        for qn in survey_qns:\n",
    "            tmp_df = condition_df[(condition_df['question_number'] == qn)]\n",
    "            mean = np.mean(tmp_df['survey_label_score'])\n",
    "            std = np.std(tmp_df['survey_label_score'])\n",
    "\n",
    "            file.write(f\"{qn}. mean: {round(mean, 3)}, std: {round(std, 3)}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
